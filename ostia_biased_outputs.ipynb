{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization with transducers\n",
    "\n",
    "In this notebook, I explore a way of extracting a FST for tokenization using the assumption that biases the input alphabet.\n",
    "\n",
    "I assume that there are two categories of symbols within the alphabet: *static* symbols and *active* symbols. \n",
    "The static symbols correspond to themselves on the side of output, such as in `a` $\\rightarrow$ `a`.\n",
    "The behavior of the active symbols needs to be determined: as in the pair `(\"stop.\", \"stop_.\")` the output of `.` is `.` $\\rightarrow$ `_.`. \n",
    "However, in `(\"mr. Bean\", \"mr. Bean\")`, `.` stays the same without introducing the space before it. \n",
    "\n",
    "I start by initializing a Prefix Tree Transducer (PTT) for the stretches of active symbols while ignoring the stretches of the static ones -- by definition, they are the same on both input and output sides. \n",
    "Every occurrence of the active symbol in the input introduces a new state.\n",
    "Also, a special symbol marks the end of line, therefore helping us to differentiate between the final and non-final active symbols stretches.\n",
    "At this stage, we only have PTT for the active symbols parts of the input strings: all the static stretches are ignored.\n",
    "\n",
    "Then I distribute the corresponding parts of the output by writing the identity symbols as themselves. If given a stretch of the moving symbols $w_{i_1}\\dots w_{i_n}$ on the input side, check which part of the corresponding output stretch $w_{o}$ in the state output of the state $w_{i_n}$. **_Any inconsistency when doing this step indicates that either the data not deterministic, or that the initial assumption about the identity symbols is wrong._**\n",
    "\n",
    "Then I am onwarding the PTT following the ONWARD-PTT algorithm from Colin de la Higuera (2010).\n",
    "Like this we are getting a PTT where every output is written as soon as possible.\n",
    "\n",
    "Last step is to rebuild the branches of the transducer in the following manner:\n",
    "   1. Collect a list of leave nodes of the current PTT.\n",
    "   2. For every leaf node, determine if it is final (ends with the EOL character) or not.\n",
    "   3. Merge all final leaves together.\n",
    "   4. Merge all non-final leaves with the initial state of the FST.\n",
    "   5. Add reflexive loops containing static symbols on the initial state of the FST.\n",
    "\n",
    "Like this, we get the transducer that writes as much as possible as soon as possible. However, the unfortunate side of the things is that this assumption is not correct for the tokenization task.\n",
    "\n",
    "## Results\n",
    "\n",
    "In the testing section (end of the notebook), I show the successful application of this idea on the simplified tokenization task.\n",
    "\n",
    "However, the overall result is not satisfying -- the initial assumption is not correct, because there is no simple way (as of now) to detect if there are *identity symbols* for this task of tokenization: substrings of the alphabetical symbols can introduce punctuations, such as `haven't` $\\rightarrow$ `have not`, `cannot` $\\rightarrow$ `can not`, etc.\n",
    "\n",
    "**For quick results, go directly in the end of the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Getting the helper functions ready\n",
    "Here, I wrote a couple of functions that are needed in different parts of the code and are not sufficiently complicated by themselves.\n",
    "  1. *lcp* returns longest common prefix of any number of input strings; used when onwarding the PTT;\n",
    "  2. *prefix* returns all prefixes of the input string; used when building the PTT;\n",
    "  3. *remove_from_prefix* removes the given prefix from the given string; used when onwarding the PTT;\n",
    "  4. *alphabetize* returns the list of symbols used in the input sample; required to build the FST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lcp(*args):\n",
    "    ''' Finds longest common prefix of unbounded number of strings. '''\n",
    "    \n",
    "    w = list(set(i for i in args if i != \"*\"))\n",
    "    if not w: raise IndexError(\"At least one non-unknown string needs to be provided.\")\n",
    "    \n",
    "    result = \"\"\n",
    "    n = min([len(x) for x in w])\n",
    "    for i in range(n):\n",
    "        if len(set(x[i] for x in w)) == 1: result += w[0][i]\n",
    "        else: break\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def prefix(w):\n",
    "    ''' Returns a list os prefixes of a word. '''\n",
    "    return [w[:i] for i in range(len(w)+1)]\n",
    "\n",
    "\n",
    "def remove_from_prefix(w, pref):\n",
    "    ''' Removes a substring from the prefix position of another string. '''\n",
    "    \n",
    "    if w.startswith(pref): return w[len(pref):]\n",
    "    elif w == \"*\": return w\n",
    "    else: raise ValueError(pref + \" is not a prefix of \" + w)\n",
    "        \n",
    "\n",
    "def alphabetize(sample):\n",
    "    \"\"\" Returns a list of symbols that are used in the input.\n",
    "        side of the sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    symbols = []\n",
    "    for i in sample:\n",
    "        for j in i[0]:\n",
    "            if j not in symbols:\n",
    "                symbols.append(j)\n",
    "    return symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Getting the template of the FST object\n",
    "First, we create the FST object and define the *rewrite* method for this class that takes a string as an input and rewrites if by going through the transitions of the current FST. The FST has the following attributes:\n",
    "  1. *Sigma*: all symbols that are used in the data;\n",
    "  2. *Alpha*: the static symbols, they are always rewritten only by themselves;\n",
    "  3. *Beta*: the active symbols, their behavior needs to be determined;\n",
    "  4. *final*: symbol that indicates the end of the string;\n",
    "  5. *idk*: symbol that is being used to indicate unknown state output;\n",
    "  6. *Q*: list of states;\n",
    "  7. *E*: list of transitions;\n",
    "  8. *stout*: dictionary of state outputs, needed for onwarding the PTT.\n",
    "  \n",
    "**Warning:** the *final* and *idk* symbols must not appear in *Sigma*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FST():\n",
    "    ''' Generic container class for the FST-related objects.\n",
    "    * Sigma: all symbols of the alphabet;\n",
    "    * Alpha: static symbols;\n",
    "    * Beta: active symbols;\n",
    "    * final: the EOS symbol;\n",
    "    * idk: the symbols for unknown state output;\n",
    "    * Q: list of states;\n",
    "    * E: list of transitions;\n",
    "    * stout: state output dictionary (not filled until later).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, Sigma, Alpha, final=\"<\", idk=\"*\"):\n",
    "        \"\"\" Initializes the FST object. \"\"\"\n",
    "        \n",
    "        self.Sigma = Sigma\n",
    "        self.Alpha = Alpha\n",
    "        self.Beta = [i for i in self.Sigma if i not in self.Alpha]\n",
    "        self.final = final\n",
    "        self.idk = idk\n",
    "        self.Q = None\n",
    "        self.E = None\n",
    "        self.stout = None\n",
    "        \n",
    "        \n",
    "    def rewrite(self, word):\n",
    "        \"\"\" Rewrites the given word with respect to\n",
    "            the learned transductions. \"\"\"\n",
    " \n",
    "        # if there are no states, there is no transducer.\n",
    "        if self.Q == None:\n",
    "            raise ValueError(\"The transducer is not constructed.\")\n",
    "            \n",
    "        current_state = \"\"\n",
    "        write = \"\"\n",
    "        for s in word+T.final:\n",
    "            for tr in T.E:\n",
    "                if tr[0] == current_state and tr[1] == s:\n",
    "                    write += tr[2]\n",
    "                    current_state = tr[3]\n",
    "        return write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Finding the static symbols stretches\n",
    "We can find the substrings, or stretches, of the active symbols based on their location with respect to the stretches of the static symbols: we know that the static symbols stay the same in the output side. First step is then to create a function that returns all stretches of active symbols stretches from the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_static_stretch(T, w):\n",
    "    \"\"\" Finds the static symbols stretches in the input string w. \"\"\"\n",
    "    \n",
    "    w = \"%r\"%w\n",
    "    stretches = []\n",
    "    current = \"\"\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(w):\n",
    "        if w[i] in T.Alpha:\n",
    "            current += w[i]\n",
    "            \n",
    "        if w[i] in T.Beta or w[i] == T.final or i == len(w)-1:\n",
    "            if current != \"\":\n",
    "                stretches.append(current)\n",
    "                current = \"\"\n",
    "        i += 1\n",
    "    \n",
    "    return stretches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Find the active stretches mapping\n",
    "Here, we need to understand how the active symbols stretches are changed.\n",
    "We can find the static stretches in the output side of the data sample, and like that we can understand which sequence of the active symbols in the output corresponds to which input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_mappings(T, pair):\n",
    "    \"\"\" Finds which stretches of active symbols in the output correspond\n",
    "        to which ones in the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    # determine \"points of stability\", or static symbol stretches\n",
    "    static_stretches = find_static_stretch(T, pair[0])\n",
    "    \n",
    "    mod_inp = pair[0]+T.final\n",
    "    mod_out = pair[1]\n",
    "    \n",
    "    inp_active = []\n",
    "    out_active = []\n",
    "    \n",
    "    # use the list of static stretches as a stack\n",
    "    while static_stretches:\n",
    "        \n",
    "        # find at which position we find the first static stretch\n",
    "        index_inp = mod_inp.find(static_stretches[0])\n",
    "        index_out = mod_out.find(static_stretches[0])\n",
    "        \n",
    "        # if on the input side the indes is not 0, it means that \n",
    "        # the active stretch precedes it: record and delete it\n",
    "        if index_inp != 0:\n",
    "            inp_active.append(mod_inp[:index_inp])\n",
    "            out_active.append(mod_out[:index_out])\n",
    "            mod_inp = mod_inp[len(inp_active[-1]):]\n",
    "            mod_out = mod_out[len(out_active[-1]):]\n",
    "        \n",
    "        # remove the active stretch to start over\n",
    "        mod_inp = mod_inp[len(static_stretches[0]):]\n",
    "        mod_out = mod_out[len(static_stretches[0]):]\n",
    "        del static_stretches[0]\n",
    "        \n",
    "    # if you are out of static stretches, check that there are no \n",
    "    # remaining active symbols\n",
    "    if len(mod_inp) != 0:\n",
    "        inp_active.append(mod_inp)\n",
    "        out_active.append(mod_out)\n",
    "        \n",
    "    assert len(inp_active) == len(out_active)\n",
    "        \n",
    "    return list(zip(inp_active, out_active))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building the PTT template\n",
    "Similarly to de la Higuera (2010), I build the PTT, but only based on the active symbols correspondences. Note that at this step, if we would not use the final symbol (`<` by default), we would lose the information about which stretches are the final ones in the original string, and which ones are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ptt(T, S):\n",
    "    \n",
    "    # assign the state outputs\n",
    "    T.stout = {}\n",
    "    for pair in S:\n",
    "        corr = list(set(find_mappings(T, pair)))\n",
    "        for c in corr:\n",
    "            #if c[0] == \"'\": print(corr, \"\\n\", pair, \"\\n\", \"$\"+c[1]+\"$\", \"\\n\")\n",
    "            if c[0] not in T.stout:\n",
    "                T.stout[c[0]] = c[1]\n",
    "            elif c[0] in T.stout and T.stout[c[0]] != c[1]:\n",
    "                #print(pair, \"\\n\", \"$\"+c[0]+\"$\", \"$\"+T.stout[c[0]]+\"$\", \"$\"+c[1]+\"$\")\n",
    "                raise ValueError(\"Inconsistent input sample.\")\n",
    "                \n",
    "    # create a list of states\n",
    "    T.Q = []\n",
    "    for i in T.stout:\n",
    "        pref = prefix(i)\n",
    "        for p in pref:\n",
    "            if p not in T.Q:\n",
    "                T.Q.append(p)\n",
    "                \n",
    "    # assign empty transitions to the leaves\n",
    "    T.E = []\n",
    "    for i in T.Q:\n",
    "        if len(i) >= 1:\n",
    "            T.E.append([i[:-1], i[-1], \"\", i])\n",
    "            \n",
    "    # add the unknown state outputs\n",
    "    for s in T.Q:\n",
    "        if s not in T.stout:\n",
    "            T.stout[s] = T.idk\n",
    "            \n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Onwarding the PTT\n",
    "I follow de la Higuera (2010) in the way the current PTT is being onwarded. The algorithm takes the previously created PTT ``T`` and makes it onward by pushing every common prefix of every output (state or transitional) closer to the root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onward_ptt(T, q, u):\n",
    "    \"\"\" Makes the PTT onward. \"\"\"\n",
    "    \n",
    "    # proceed as deep as possible\n",
    "    for tr in T.E:\n",
    "        if tr[0] == q:\n",
    "            T, qx, w = onward_ptt(T, tr[3], tr[1])\n",
    "            if tr[2] != \"*\":\n",
    "                tr[2] += w\n",
    "                  \n",
    "    # find lcp of all ways of leaving state 1 or stopping in it\n",
    "    t = [tr[2] for tr in T.E if tr[0] == q]\n",
    "    f = lcp(T.stout[q], *t)\n",
    "    \n",
    "    # remove from the prefix unless it's the initial state\n",
    "    if f != \"\" and q != \"\":\n",
    "        for tr in T.E:\n",
    "            if tr[0] == q:\n",
    "                tr[2] = remove_from_prefix(tr[2], f)\n",
    "        T.stout[q] = remove_from_prefix(T.stout[q], f)\n",
    "                \n",
    "    return T, q, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Redirect branches\n",
    "Now that the active symbols-based PTT is built and onward, we can do 3 following changes:\n",
    "  1. All non-final leaves should loop back into the initial states;\n",
    "  2. All final leaves need to be unified in the single final state;\n",
    "  3. Reflexive transitions for the static symbols need to be added in the final state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redirect_branches(T):\n",
    "    \"\"\" Finalizes the transducer by redirecting the leaf transitions. \"\"\"\n",
    "\n",
    "    nf, f = [], []\n",
    "    \n",
    "    # find all leaf nodes\n",
    "    for s in T.Q:\n",
    "        appears = False\n",
    "        for tr in T.E:\n",
    "            if tr[0] == s:\n",
    "                appears = True\n",
    "                break\n",
    "                \n",
    "        # detect if the leaf state is final or not\n",
    "        if appears == False:\n",
    "            if s.endswith(T.final):\n",
    "                f.append(s)\n",
    "            else:\n",
    "                nf.append(s)\n",
    "\n",
    "    for tr in T.E:\n",
    "        # redirect non-final leaves into the initial state\n",
    "        if tr[3] in nf:\n",
    "            nf.remove(tr[3])\n",
    "            tr[3] = \"\"\n",
    "            \n",
    "        # merge all the final leaves into a single one\n",
    "        elif tr[3] in f:\n",
    "            f.remove(tr[3])\n",
    "            T.Q.remove(tr[3])\n",
    "            tr[3] = T.final\n",
    "            if T.final not in T.Q:\n",
    "                T.Q.append(T.final)\n",
    "    \n",
    "    # add reflexive transitions for the static symbols\n",
    "    for s in T.Alpha:\n",
    "        T.E.append([\"\", s, s, \"\"])\n",
    "    \n",
    "    # kill the stout: not needed anymore\n",
    "    T.stout = None\n",
    "                \n",
    "    return T\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Putting it all together\n",
    "In the next cell, I put all of the function written above together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fstok(S, Alpha):\n",
    "    \"\"\" Algoritm that lears the tokenization rules. \"\"\"\n",
    "    \n",
    "    # provide the alphabets and initialize the FST\n",
    "    Sigma = alphabetize(S)\n",
    "    T = FST(Sigma, Alpha)\n",
    "    \n",
    "    # build the active part of the PTT\n",
    "    T = build_ptt(T, S)\n",
    "    T = onward_ptt(T, \"\", \"\")[0]\n",
    "    \n",
    "    # redirect the branches of the PTT\n",
    "    T = redirect_branches(T)\n",
    "    \n",
    "    return T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: toy data sample\n",
    "\n",
    "In this section, I test the proposed algorithm given the toy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [(\"Hello, Jon.\", \"Hello , Jon .\"),\n",
    "     (\"Because.\", \"Because .\"),\n",
    "     (\"Come in, dear friends.\", \"Come in , dear friends .\"),\n",
    "     (\"Apples, bananas, kiwis.\", \"Apples , bananas , kiwis .\"),\n",
    "     (\"Mr. Bean came home.\", \"Mr. Bean came home .\"),\n",
    "     (\"What are you talking about?\", \"What are you talking about ?\"),\n",
    "     (\"What are you talking about?!\", \"What are you talking about ?!\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Alpha = list(string.ascii_lowercase+string.ascii_uppercase)\n",
    "Alpha.extend([str(i) for i in range(10)])\n",
    "T = fstok(S, Alpha)\n",
    "#print(\"States:\", T.Q)\n",
    "#print(\"Transitions:\", T.E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, dear. ---> Hello ,  dear .\n",
      "Mr. Bean is nice. ---> Mr. Bean is nice .\n",
      "Apples, bananas, and oranges are tasty. ---> Apples ,  bananas ,  and oranges are tasty .\n",
      "So what?! ---> So what ?!\n"
     ]
    }
   ],
   "source": [
    "test = [\"Hello, dear.\", \"Mr. Bean is nice.\", \n",
    "        \"Apples, bananas, and oranges are tasty.\", \"So what?!\"]\n",
    "for i in test:\n",
    "    print(i, \"--->\", T.rewrite(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a large corpus, this way needs adjustments. In the WSJ data, the assumption that the alphabetical symbols are always rewritten as themselves is not correct.\n",
    "\n",
    "For example, in the pair (`'Andy'`, `'Andy'`), the rule is `'` $\\rightarrow$ `'`. However, in (`\"Andy's\"`, `\"Andy 's\"`), it is `'` $\\rightarrow$ `_'`. Therefore, in order to account for this situation, we need to check if `s` follows the hyphen, therefore `s` will decide the output and not just `'`.\n",
    "\n",
    "Removing `s` from _Alpha_ does not really save the situation, because then the same problem appears with the other sentence. In the pair (`we're`, `we 're`), it is the symbol `r` that decides the behavior of the hyphen.\n",
    "\n",
    "And removing `r` would not help either: there are other examples.\n",
    "   * `hadn't` $\\rightarrow$ `had n't`\n",
    "   * `cannot` $\\rightarrow$ `can not`\n",
    "   * $\\dots$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
